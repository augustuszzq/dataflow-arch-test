// This program implements the SUMMA matrix multiplication algorithm and is
// written as an example to show how to use the `collectives_2d` library.

// We perform GEMM in `P` many steps on a grid of `P x P` processors.
// At each step `i`, PEs in the `i`th column broadcast their home tiles of `A`
// to other PEs in their row, and PEs in the `i`th row broadcast their home
// tiles of `B` to other PEs in their column. Once both broadcasts are complete
// as determined by `x_done()` and `y_done()` both being activated,
// each PE computes `C_tile += Ap * Bp` where `Ap` and `Bp` are pointers to
// either the PE's home tile or the tile it received through broadcasts.

param memcpyParams:comptime_struct;
param c2dParams:comptime_struct;

// Matrix size params
param Mt: u16;

param Nt: u16;

param Q_tile:*[Mt*Nt]f32;
param K_tile:*[Mt*Nt]f32;
var QK_tile = @zeros([Mt*Mt]f32);
var QK : *[Mt*Mt]f32;
// Task IDs
const EXIT:            local_task_id = @get_local_task_id(12);
const compute_task_id: local_task_id = @get_local_task_id(13);
const x_task_id:       local_task_id = @get_local_task_id(14);
const y_task_id:       local_task_id = @get_local_task_id(15);



const mpi_x = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2dParams.x,
    .queues = [2]u16{2,4},
    .dest_dsr_ids = [1]u16{1},
    .src0_dsr_ids = [1]u16{1},
    .src1_dsr_ids = [1]u16{1}
    });
const mpi_y = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2dParams.y,
    .queues = [2]u16{3,5},
    .dest_dsr_ids = [1]u16{2},
    .src0_dsr_ids = [1]u16{2},
    .src1_dsr_ids = [1]u16{2}
    });

// On WSE-2, memcpy uses input/output queue 0
// On WSE-3, memcpy uses input/output queues 0 and 1
const sys_mod = @import_module("<memcpy/memcpy>", memcpyParams);

const P = @get_rectangle().width;

// This PE's home tile of A, B, C
// `A_tile` and `B_tile` will be populated with initial values by run.py
// These arrays are stored in a column major format.




var Q_buffer = @zeros([Mt*Nt]f32);
var K_buffer = @zeros([Mt*Nt]f32);

var px: u16;
var py: u16;

task x_done() void {
  @activate(compute_task_id);
}

task y_done() void {
  @unblock(compute_task_id);
}

var step: u16 = 0;
fn main() void {
  @assert(step < P);

  // The first time through we need to initialize our state
  if (step == 0) {
    mpi_x.init();
    mpi_y.init();
    px = mpi_x.pe_id;
    py = mpi_y.pe_id;
  }

  // Communicate along both rows and columns
  const Ap = if (px == step) Q_tile else &Q_buffer;
  const Bp = if (py == step) K_tile else &K_buffer;
  mpi_x.broadcast(step, @ptrcast([*]u32, Ap), Mt * Nt, x_task_id);
  mpi_y.broadcast(step, @ptrcast([*]u32, Bp), Nt * Mt, y_task_id);
}

task compute() void {
  const Ap = if (px == step) Q_tile else &Q_buffer;
  const Bp = if (py == step) K_tile else &K_buffer;

  // Do an fmacs based local GEMM
  var A_dsd  = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> Q_tile[i] });
  A_dsd = @set_dsd_base_addr(A_dsd, Ap);

  for (@range(i16, Nt)) |k| {
    var C_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> QK_tile[i] });

    for (@range(i16, Mt)) |j| {

      const b = Bp.*[j*@bitcast(i16,Nt) + k];
      @fmacs(C_dsd, C_dsd, A_dsd, b);
      C_dsd = @increment_dsd_offset(C_dsd, Mt, f32);
    }
    A_dsd = @increment_dsd_offset(A_dsd, Mt, f32);
  }

  step += 1;
  @block(compute_task_id);

  if (step != P) {
    main();
  } else {
    for(@range(i16,Mt*Mt))|i|{
      (QK.*)[i] = QK_tile[i];
    }
    @activate(EXIT);
  }
}
fn qkmm(C:*[Mt*Mt]f32) void {
  QK = C;
  main();

}
task f_exit() void {
  // the user must unblock cmd color for every PE
  sys_mod.unblock_cmd_stream();
}
comptime {
  @bind_local_task(f_exit, EXIT);
  @bind_local_task(compute, compute_task_id);
  @bind_local_task(x_done, x_task_id);
  @bind_local_task(y_done, y_task_id);
  @block(compute_task_id);
}